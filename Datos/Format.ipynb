{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reunir datos en una unica version por sector\n",
    "sector = ['01','02','03','04','05','06','07','08','09','11','12']\n",
    "versiones = [2,2,1,1,10,2,1,1,1,1,1]\n",
    "for i in range(len(sector)):\n",
    "    archivo_base = 'Tagged_Tweets_' + sector[i]\n",
    "    archivo_final =  'Tweets_' + sector[i] + '.csv'\n",
    "    if versiones[i] == 1:\n",
    "        df = pd.read_csv(archivo_base + '.csv', index_col=0)\n",
    "        df.to_csv(archivo_final, index=True, header=True)\n",
    "    else:\n",
    "        df = pd.read_csv(archivo_base + '_' + str(1) + '.csv', index_col=0)\n",
    "        df.to_csv(archivo_final, index=True, header=True)\n",
    "        for j in range(1,versiones[i]):\n",
    "            archivo = archivo_base + '_' + str(j+1) + '.csv'\n",
    "            tweet_df = pd.read_csv(archivo, index_col=0)\n",
    "            tweet_df.to_csv(archivo_final, mode='a', index=True, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar columnas descartadas + formatear ciertas columnas\n",
    "def FormatearDatos(nombre_archivo, sector):\n",
    "    tweet_df = pd.read_csv(nombre_archivo, index_col=0)\n",
    "    tweet_df.drop([\"cashtags\", \"username\"], inplace=True, axis=1)\n",
    "    tweet_df.reply_to = tweet_df.reply_to.map({\"{\\'user_id\\': None, \\'username\\': None}\": 'No'}).fillna(\"Si\").astype(str)\n",
    "    tweet_df.place = tweet_df.place.map({np.nan: \"No\"}).fillna(\"Si\").astype(str)\n",
    "    tweet_df[\"sector\"] = sector\n",
    "    dia_semana = []\n",
    "    dia_semana.clear()\n",
    "    lemma_tag = []\n",
    "    lemma_tag.clear()\n",
    "    clean_tag = []\n",
    "    clean_tag.clear()\n",
    "    short = []\n",
    "    short.clear()\n",
    "    for date, lemma, clean, tweet in zip(tweet_df[\"date\"], tweet_df[\"lemma_tag\"], tweet_df[\"clean_tag\"], tweet_df[\"clean_tweet\"]):\n",
    "        dateframe = pd.Timestamp(date).dayofweek\n",
    "        if dateframe == 0:\n",
    "            dateframe = \"Lunes\"\n",
    "        if dateframe == 1:\n",
    "            dateframe = \"Martes\"\n",
    "        if dateframe == 2:\n",
    "            dateframe = \"Miércoles\"\n",
    "        if dateframe == 3:\n",
    "            dateframe = \"Jueves\"\n",
    "        if dateframe == 4:\n",
    "            dateframe = \"Viernes\"\n",
    "        if dateframe == 5:\n",
    "            dateframe = \"Sabado\"\n",
    "        if dateframe == 6:\n",
    "            dateframe = \"Domingo\"\n",
    "        dia_semana.append(dateframe)\n",
    "        lemma = lemma.replace(\"[\", '')\n",
    "        lemma = lemma.replace(\"]\", '')\n",
    "        lemma = lemma.replace(\" \", '')\n",
    "        lemma_tag.append(lemma)\n",
    "        clean = clean.replace(\"[\", '')\n",
    "        clean = clean.replace(\"]\", '')\n",
    "        clean = clean.replace(\" \", '')\n",
    "        clean_tag.append(clean)\n",
    "        palabras = tweet.split(\",\")\n",
    "        if len(palabras) < 3:\n",
    "            short.append(\"Si\")\n",
    "        else:\n",
    "            short.append(\"No\")\n",
    "    tweet_df[\"dia\"] = dia_semana\n",
    "    tweet_df[\"lemma_tag\"] = lemma_tag\n",
    "    tweet_df[\"clean_tag\"] = clean_tag\n",
    "    tweet_df[\"short\"] = short\n",
    "    tweet_df.to_csv(nombre_archivo, index=True, header=True)\n",
    "        \n",
    "sector = ['01','02','03','04','05','06','07','08','09','11','12']\n",
    "for i in range(len(sector)):\n",
    "    archivo_base = 'Tweets_' + sector[i] + '.csv'\n",
    "    FormatearDatos(archivo_base, sector[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar y eliminar datos duplicados\n",
    "sector = ['01','02','03','04','05','06','07','08','09','11','12']\n",
    "for i in range(len(sector)-1,1,-1):\n",
    "    nombre_archivo = 'Tweets_' + sector[i] + '.csv'\n",
    "    tweet_df = pd.read_csv(nombre_archivo, index_col=0)\n",
    "    if i != 6:\n",
    "        aux_name = 'Tweets_' + sector[i-1] + '.csv'\n",
    "        aux_df = pd.read_csv(aux_name, index_col=0)\n",
    "        # eliminar copias existentes\n",
    "        duplicados = pd.merge(tweet_df, aux_df, how='inner',on='created_at',left_index=True)\n",
    "        tweet_df = tweet_df.drop(duplicados.index)\n",
    "    elif i == 7:\n",
    "        continue\n",
    "    else:\n",
    "        aux_name2 = 'Tweets_' + sector[i+1] + '.csv'\n",
    "        aux_df2 = pd.read_csv(aux_name2, index_col=0)\n",
    "        # eliminar copias existentes\n",
    "        duplicados2 = pd.merge(tweet_df, aux_df2, how='inner',on='created_at', left_index=True)\n",
    "        tweet_df = tweet_df.drop(duplicados.index)\n",
    "    tweet_df.to_csv(nombre_archivo, index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reunir datos de todos los sectores en una unica version\n",
    "sector = ['01','02','03','04','05','06','07','08','09','11','12']\n",
    "archivo_base =  'Tweets_' + sector[0] + '.csv'\n",
    "df = pd.read_csv(archivo_base, index_col=0)\n",
    "df.to_csv('Tweets.csv' , index=True, header=True)\n",
    "for i in range(1,len(sector)):\n",
    "    archivo = 'Tweets_' + sector[i] + '.csv'\n",
    "    tweet_df = pd.read_csv(archivo, index_col=0)\n",
    "    tweet_df.to_csv('Tweets.csv', mode='a', index=True, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 50635), (\"'coronavirus'\", 17456), (\"'covid_19'\", 15381), (\"'quedateencasa'\", 4084), (\"'chile'\", 3436), (\"'covid19'\", 2489), (\"'cuidémonosentretodos'\", 2396), (\"'cuarentena'\", 2321), (\"'covid'\", 1865), (\"'covid__19'\", 1722), (\"'covidー19'\", 1598), (\"'coronaviruschile'\", 1289), (\"'covid19chile'\", 1179), (\"'coronavirusenchile'\", 1064), (\"'cuarentenatotal'\", 996), (\"'plancoronavirus'\", 977), (\"'pandemia'\", 811), (\"'covidー19chile'\", 787), (\"'covid2019'\", 710), (\"'iquique'\", 681), (\"'antofagasta'\", 652), (\"'yomequedoencasa'\", 617), (\"'covid2019chile'\", 585), (\"'quédateencasa'\", 545), (\"'cuidemonosentretodos'\", 507), (\"'quedateentucasa'\", 463), (\"'tarapacá'\", 442), (\"'ohiggins'\", 439), (\"'valdiviacl'\", 435), (\"'cuarentenatotalchile'\", 406), (\"'cuarentenacoronavirus'\", 376), (\"'cuarentenanacional'\", 363), (\"'viñadelmar'\", 333), (\"'toquedequeda'\", 323), (\"'altohospicio'\", 321), (\"'quedateenlacasa'\", 320), (\"'renunciapiñera'\", 290), (\"'yoapruebo'\", 283), (\"'laserena'\", 279), (\"'talca'\", 274), (\"'santiago'\", 266), (\"'mañalich'\", 261), (\"'rancagua'\", 260), (\"'bienvenidos13'\", 252), (\"'contigochv'\", 237), (\"'coronacrisis'\", 234), (\"'piñera'\", 228), (\"'renunciamañalich'\", 225), (\"'apruebo'\", 223), (\"'calama'\", 222)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    for token in cleaned_tokens_list:\n",
    "            yield token\n",
    "            \n",
    "# Obtener todos los Hashtags presentes en cada uno de los datos por separado\n",
    "archivo =  'Tweets.csv'\n",
    "tuplas = []\n",
    "tuplas.clear()\n",
    "lista_hashtags = []\n",
    "lista_hashtags.clear()\n",
    "df = pd.read_csv(archivo, index_col=0)\n",
    "for hashtags,sec,ltag,ctag,nlik,nrep,nret in zip(df['hashtags'],df['sector'],df['lemma_tag'],df['clean_tag'],df['nlikes'],df['nreplies'],df['nretweets']):\n",
    "    hashtags = hashtags.replace('[','')\n",
    "    hashtags = hashtags.replace(']','')\n",
    "    hashtags = hashtags.replace(' ','')\n",
    "    hashtag = hashtags.split(',')\n",
    "    for tag in hashtag:\n",
    "        lista_hashtags.append(tag)\n",
    "        tuplas.append([tag,str(ltag),str(ctag),nlik,nrep,nret,sec])\n",
    "\n",
    "all_hashtags = get_all_words(lista_hashtags)\n",
    "freq_hashtags = FreqDist(all_hashtags)\n",
    "list_hashtags = freq_hashtags.most_common(50)\n",
    "print(list_hashtags)\n",
    "        \n",
    "'''df = pd.DataFrame(tuplas, columns = ['hashtags', 'lemma_tag', 'clean_tag', 'nlikes', 'nreplies', 'nretweets', 'sector'])  \n",
    "df.to_csv('Hashtags.csv', index=True, header=False)'''\n",
    "\n",
    "tuplas_hashtags = []\n",
    "tuplas_hashtags.clear()\n",
    "for i in range(50):\n",
    "    tuplas_hashtags.append([list_hashtags[i]])\n",
    "df = pd.DataFrame(tuplas_hashtags, columns = ['hashtag'])  \n",
    "df.to_csv('HashtagsComunes.csv', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo =  'Tweets.csv'\n",
    "clean_class = []\n",
    "clean_class.clear()\n",
    "lemma_class = []\n",
    "lemma_class.clear()\n",
    "df = pd.read_csv(archivo, index_col=0)\n",
    "for ltag,ctag in zip(df['lemma_tag'],df['clean_tag']):\n",
    "    # Agregar clasificacion (etiqueta) adecuada mas exigente y filtrando datos neutros y no clasificables\n",
    "    tag = ctag.split(',')\n",
    "    if int(tag[0]) > (int(tag[1]) + 2):\n",
    "        clean_class.append('Positivo')\n",
    "    elif (int(tag[0]) + 2) < int(tag[1]):\n",
    "        clean_class.append('Negativo')\n",
    "    elif int(tag[0]) < 3 and int(tag[1]) < 3:\n",
    "        clean_class.append('No Clasificable')\n",
    "    else:\n",
    "        clean_class.append('Neutro')\n",
    "    tag = ltag.split(',')\n",
    "    if int(tag[0]) > (int(tag[1]) + 2):\n",
    "        lemma_class.append('Positivo')\n",
    "    elif (int(tag[0]) + 2) < int(tag[1]):\n",
    "        lemma_class.append('Negativo')\n",
    "    elif int(tag[0]) < 3 and int(tag[1]) < 3:\n",
    "        lemma_class.append('No Clasificable')\n",
    "    else:\n",
    "        lemma_class.append('Neutro')\n",
    "df['clean_class'] = clean_class\n",
    "df['lemma_class'] = lemma_class\n",
    "df.drop([\"date\", \"place\", \"language\", \"nlikes\", \"nreplies\", \"nretweets\", \"reply_to\", \"dia\", \"short\"], inplace=True, axis=1)\n",
    "df.to_csv('Tweets-2.csv', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo =  'Tweets.csv'\n",
    "clean_class = []\n",
    "clean_class.clear()\n",
    "lemma_class = []\n",
    "lemma_class.clear()\n",
    "df = pd.read_csv(archivo, index_col=0)\n",
    "for ltag,ctag in zip(df['lemma_tag'],df['clean_tag']):\n",
    "    # Agregar clasificacion (etiqueta) adecuada mas exigente y filtrando datos neutros y no clasificables\n",
    "    tag = ctag.split(',')\n",
    "    if int(tag[0]) > (int(tag[1])):\n",
    "        clean_class.append('Positivo')\n",
    "    elif (int(tag[0])) < int(tag[1]):\n",
    "        clean_class.append('Negativo')\n",
    "    elif int(tag[0]) < 1 and int(tag[1]) < 1:\n",
    "        clean_class.append('No Clasificable')\n",
    "    else:\n",
    "        clean_class.append('Neutro')\n",
    "    tag = ltag.split(',')\n",
    "    if int(tag[0]) > (int(tag[1])):\n",
    "        lemma_class.append('Positivo')\n",
    "    elif (int(tag[0])) < int(tag[1]):\n",
    "        lemma_class.append('Negativo')\n",
    "    elif int(tag[0]) < 1 and int(tag[1]) < 1:\n",
    "        lemma_class.append('No Clasificable')\n",
    "    else:\n",
    "        lemma_class.append('Neutro')\n",
    "df['clean_class'] = clean_class\n",
    "df['lemma_class'] = lemma_class\n",
    "# df.drop([\"date\", \"place\", \"language\", \"nlikes\", \"nreplies\", \"nretweets\", \"reply_to\", \"dia\", \"short\"], inplace=True, axis=1)\n",
    "df.to_csv('Tweets.csv', index=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
